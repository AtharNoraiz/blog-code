{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# plotting params\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 12\n",
    "plt.rcParams['figure.figsize'] = (13.0, 6.0)\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = False\n",
    "\n",
    "device = torch.device(\"cuda\" if GPU else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure shuffling is turned off!\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if GPU else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        out = F.relu(F.max_pool2d(self.conv2(out), 2))\n",
    "        out = out.view(-1, 320)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_gradient(losses, model):\n",
    "    \"\"\"\n",
    "    Compute the L2 norm of the gradient of the loss \n",
    "    with respect to the weights and biases of the network.\n",
    "    \n",
    "    Since there's a weight and bias vector associated with \n",
    "    every convolutional and fully-connected layer, the square\n",
    "    root of the sum of the squared gradient norms is returned.\n",
    "    \"\"\"\n",
    "    norms = []\n",
    "    for l in losses:\n",
    "        grad_params = torch.autograd.grad(l, model.parameters(), create_graph=True)\n",
    "        grad_norm = 0\n",
    "        for grad in grad_params:\n",
    "            grad_norm += grad.norm(2).pow(2)\n",
    "        norms.append(grad_norm.sqrt())\n",
    "    return norms\n",
    "\n",
    "def accuracy(predicted, ground_truth):\n",
    "    predicted = torch.max(predicted, 1)[1]\n",
    "    total = len(ground_truth)\n",
    "    correct = (predicted == ground_truth).sum().double()\n",
    "    acc = 100 * (correct / total)\n",
    "    return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_stats = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        acc = accuracy(output, target)\n",
    "        \n",
    "        # compute batch loss and gradient norm\n",
    "        losses = F.nll_loss(output, target, reduction='none')\n",
    "        grad_norms = accumulate_gradient(losses, model)\n",
    "        indices = [batch_idx*len(data) + i for i in range(len(data))]\n",
    "        \n",
    "        batch_stats = []\n",
    "        for l, g, i in zip(losses, grad_norms, indices):\n",
    "            batch_stats.append([i, [g, l]])\n",
    "        epoch_stats.append(batch_stats)\n",
    "            \n",
    "        # take average loss and accuracy\n",
    "        loss = losses.mean()\n",
    "        \n",
    "        # backwards pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 25 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.2f}%'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "100. * batch_idx / len(train_loader), loss.item(), acc))\n",
    "\n",
    "    return epoch_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.633006\tAcc: 10.94%\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 1.619076\tAcc: 48.44%\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.823937\tAcc: 71.88%\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.598104\tAcc: 79.69%\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.370156\tAcc: 92.19%\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.533670\tAcc: 81.25%\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.291023\tAcc: 89.06%\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 0.611064\tAcc: 82.81%\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.301691\tAcc: 85.94%\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.171518\tAcc: 96.88%\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.311606\tAcc: 92.19%\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.311102\tAcc: 90.62%\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.225483\tAcc: 93.75%\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.140306\tAcc: 93.75%\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.154456\tAcc: 95.31%\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.203898\tAcc: 93.75%\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.032903\tAcc: 100.00%\n",
      "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 0.218158\tAcc: 93.75%\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.071584\tAcc: 98.44%\n",
      "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 0.053365\tAcc: 100.00%\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.092851\tAcc: 96.88%\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.023382\tAcc: 100.00%\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.239355\tAcc: 93.75%\n",
      "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 0.159463\tAcc: 93.75%\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.054997\tAcc: 98.44%\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.179467\tAcc: 93.75%\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.061505\tAcc: 96.88%\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.169429\tAcc: 95.31%\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.159596\tAcc: 95.31%\n",
      "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 0.468967\tAcc: 87.50%\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.086294\tAcc: 95.31%\n",
      "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 0.220987\tAcc: 93.75%\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.208579\tAcc: 93.75%\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.264812\tAcc: 93.75%\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.046534\tAcc: 96.88%\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.117640\tAcc: 98.44%\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.196920\tAcc: 93.75%\n",
      "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 0.011510\tAcc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model = SmallConv().to(device)\n",
    "\n",
    "# relu init\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
    "\n",
    "# define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "stats = []\n",
    "for epoch in range(1,  num_epochs+1):\n",
    "    stats.append(train(model, device, train_loader, optimizer, epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_vs_gradnorm(list_stats):\n",
    "    flattened = [val for sublist in list_stats for val in sublist]\n",
    "    sorted_idx = sorted(range(len(flattened)), key=lambda k: flattened[k][1][0])\n",
    "    losses = [flattened[idx][1][1] for idx in sorted_idx]\n",
    "    losses = [l.item() for l in losses]\n",
    "    return losses\n",
    "\n",
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_losses = np.array(loss_vs_gradnorm(stats[0]))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13.0, 6.0))\n",
    "\n",
    "rolling_mean = np.mean(rolling_window(sorted_losses, 50), 1)\n",
    "rolling_std = np.std(rolling_window(sorted_losses, 50), 1)\n",
    "\n",
    "plt.plot(range(len(rolling_mean)), rolling_mean, alpha=0.98, linewidth=0.9)\n",
    "plt.fill_between(range(len(rolling_std)), rolling_mean-rolling_std, rolling_mean+rolling_std, alpha=0.5)\n",
    "\n",
    "plt.title('Sorted Loss w.r.t Gradient Norm')\n",
    "plt.xlabel('Sorted Sample #')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./loss_vs_grad.png\", format=\"png\", dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
