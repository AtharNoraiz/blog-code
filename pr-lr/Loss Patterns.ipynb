{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import reduce\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "# plotting params\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 12\n",
    "plt.rcParams['figure.figsize'] = (13.0, 6.0)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "data_dir = './data/'\n",
    "plot_dir = './imgs/'\n",
    "dump_dir = './dump/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = False\n",
    "\n",
    "device = torch.device(\"cuda\" if GPU else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if GPU else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "\n",
    "We need to create a special dataloader for the experiment with shuffling. This is necessary because we need to keep track of each sample and shuffling loses that information.\n",
    "\n",
    "To solve this, we can:\n",
    "\n",
    "- create permutations of a list of numbers from 0 to 59,999 (the number of images in MNIST)\n",
    "- create a sampler class that takes a list and interates over it sequentially\n",
    "- at each epoch, create a dataloader with a sampler that gets fed the precomputed permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSampler(Sampler):\n",
    "    def __init__(self, idx):\n",
    "        self.idx = idx\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(data_dir, batch_size, permutation=None, num_workers=3, pin_memory=False):\n",
    "    normalize = transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "    transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "    dataset = datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "    \n",
    "    sampler = None\n",
    "    if permutation is not None:\n",
    "        sampler = LinearSampler(permutation)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size,\n",
    "        shuffle=False, num_workers=num_workers,\n",
    "        pin_memory=pin_memory, sampler=sampler\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        out = F.relu(F.max_pool2d(self.conv2(out), 2))\n",
    "        out = out.view(-1, 320)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted, ground_truth):\n",
    "    predicted = torch.max(predicted, 1)[1]\n",
    "    total = len(ground_truth)\n",
    "    correct = (predicted == ground_truth).sum().double()\n",
    "    acc = 100 * (correct / total)\n",
    "    return acc.item()\n",
    "\n",
    "def loss_vs_gradnorm(list_stats):\n",
    "    flattened = [val for sublist in list_stats for val in sublist]\n",
    "    sorted_idx = sorted(range(len(flattened)), key=lambda k: flattened[k][1][0])\n",
    "    losses = [flattened[idx][1][1].item() for idx in sorted_idx]\n",
    "    return losses\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_stats = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        acc = accuracy(output, target)\n",
    "        \n",
    "        # compute batch loss and gradient norm\n",
    "        losses = F.nll_loss(output, target, reduction='none')\n",
    "        indices = [batch_idx*len(data) + i for i in range(len(data))]\n",
    "        \n",
    "        batch_stats = []\n",
    "        for i, l in zip(indices, losses):\n",
    "            batch_stats.append([i, l])\n",
    "        epoch_stats.append(batch_stats)\n",
    "            \n",
    "        # take average loss\n",
    "        loss = losses.mean()\n",
    "        \n",
    "        # backwards pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 25 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.2f}%'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "100. * batch_idx / len(train_loader), loss.item(), acc))\n",
    "\n",
    "    return epoch_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "learning_rate = 1e-3\n",
    "mom = 0.99\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "model = SmallConv().to(device)\n",
    "\n",
    "# relu init\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
    "\n",
    "# define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=mom)\n",
    "train_loader = get_data_loader(data_dir, batch_size, None, **kwargs)\n",
    "\n",
    "stats_no_shuffling = []\n",
    "for epoch in range(1,  num_epochs+1):\n",
    "    stats_no_shuffling.append(train(model, device, train_loader, optimizer, epoch))\n",
    "pickle.dump(stats_no_shuffling, open(\"./no_shuffling.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "learning_rate = 1e-3\n",
    "mom = 0.99\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create permutations\n",
    "permutations = []\n",
    "permutations.append(list(np.arange(60000)))\n",
    "\n",
    "x = list(np.arange(60000))\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for _ in range(num_epochs-1):\n",
    "    np.random.shuffle(x)\n",
    "    permutations.append(x.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "model = SmallConv().to(device)\n",
    "\n",
    "# relu init\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
    "\n",
    "# define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=mom)\n",
    "\n",
    "stats_with_shuffling = []\n",
    "for epoch in range(1,  num_epochs+1):\n",
    "    train_loader = get_data_loader(data_dir, batch_size, permutations[epoch-1], **kwargs)\n",
    "    stats_with_shuffling.append(train(model, device, train_loader, optimizer, epoch))\n",
    "pickle.dump(stats_with_shuffling, open(\"./with_shuffling.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pickle.load(open(dump_dir + \"no_shuffling_mnist.p\", \"rb\"))\n",
    "# permutations = pickle.load(open(dump_dir + \"permutations_mnist.p\", \"rb\"))\n",
    "permutations = [np.arange(60000)]*len(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab losses and grads over all epochs\n",
    "grads = []\n",
    "losses = []\n",
    "\n",
    "for s in stats:\n",
    "    g, l = losses_grads(s)\n",
    "    grads.append(g)\n",
    "    losses.append(l)\n",
    "    \n",
    "grads = [item for sublist in grads for item in sublist]\n",
    "losses = [item for sublist in losses for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorted Indices Quantile Distribution\n",
    "\n",
    "Track the evolution of sorted indices across time. Use 10 quantiles for example, bin indices into quantiles, and examine mean and variance of quantile for each sample. This will help me get an idea of how stable the importance of a sample is. If it is important in the initial epochs, does this hold true in future epochs or is this random?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap the indices based on the `permutations` list\n",
    "fixed = []\n",
    "for i in range(len(stats)):\n",
    "    s = stats[i]\n",
    "    flattened = [val for sublist in s for val in sublist]\n",
    "    for j in range(len(flattened)):\n",
    "        flattened[j][0] = permutations[i][j]\n",
    "    fixed.append(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resort in increasing index order\n",
    "for i in range(len(fixed)):\n",
    "    fixed[i] = sorted(fixed[i], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_split(seq, percentages):\n",
    "    cdf = np.cumsum(percentages)\n",
    "    assert np.allclose(cdf[-1], 1.0)\n",
    "    stops = list(map(int, cdf * len(seq)))\n",
    "    return [seq[a:b] for a, b in zip([0]+stops, stops)]\n",
    "\n",
    "def idx_evolution(all_epochs):\n",
    "    percentile_splits = []\n",
    "    for epoch in all_epochs:\n",
    "        # sort in descending order based on loss\n",
    "        sorted_loss_idx = sorted(range(len(epoch)), key=lambda k: epoch[k][1][0,1], reverse=True)\n",
    "        splits = percentage_split(sorted_loss_idx, [0.1]*10)\n",
    "        percentile_splits.append(splits)\n",
    "    return percentile_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get percentile splits for all 5 epochs\n",
    "percentile_splits = idx_evolution(fixed)\n",
    "num_quantiles = len(percentile_splits[0])\n",
    "\n",
    "percent_matches = []\n",
    "# for each quantile\n",
    "for i in range(num_quantiles):\n",
    "    percentile_all = []\n",
    "    # decide over how many epochs to compare\n",
    "    for j in range(1, len(percentile_splits)):\n",
    "        percentile_all.append(percentile_splits[j][i])\n",
    "    matching = reduce(np.intersect1d, percentile_all)\n",
    "    percent = 100 * len(matching) / len(percentile_all[0])\n",
    "    percent_matches.append(percent)\n",
    "    \n",
    "for perc in percent_matches:\n",
    "    print(\"{0:.2f}%\".format(perc), end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.reset_orig()\n",
    "fig, ax = plt.subplots(figsize=(3,4))\n",
    "ax.bar(range(1, len(percent_matches)+1), percent_matches, width=0.9, color='r')\n",
    "ax.set_xlabel('Quantile')\n",
    "ax.set_ylabel('Percent Match Across Epochs')\n",
    "ax.set_title('All Epochs (No Shuffling)')\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_dir + \"percent_match_no_shuffling_all.png\", format=\"png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_size = 60000\n",
    "cifar_size = 50000\n",
    "\n",
    "permutations = []\n",
    "permutations.append(list(np.arange(mnist_size)))\n",
    "x = list(np.arange(mnist_size))\n",
    "np.random.seed(seed)\n",
    "for i in range(4):\n",
    "    np.random.shuffle(x)\n",
    "    permutations.append(x.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "stats = []\n",
    "for epoch in range(1,  num_epochs+1):\n",
    "    train_loader = get_train_loader_random(\n",
    "        data_dir, dataset, batch_size, permutations[epoch-1],\n",
    "        **kwargs\n",
    "    )\n",
    "    stats.append(train(epoch, train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(permutations, open(dump_dir + \"permutations_mnist.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
