{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import reduce\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import Sampler, WeightedRandomSampler\n",
    "\n",
    "# plotting params\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 12\n",
    "plt.rcParams['figure.figsize'] = (13.0, 6.0)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "data_dir = './data/'\n",
    "plot_dir = './imgs/'\n",
    "dump_dir = './dump/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = False\n",
    "\n",
    "device = torch.device(\"cuda\" if GPU else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if GPU else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSampler(Sampler):\n",
    "    def __init__(self, idx):\n",
    "        self.idx = idx\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(data_dir, batch_size, permutation=None, num_workers=3, pin_memory=False):\n",
    "    normalize = transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "    transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "    dataset = MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "    \n",
    "    sampler = None\n",
    "    if permutation is not None:\n",
    "        sampler = LinearSampler(permutation)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset, batch_size=batch_size,\n",
    "        shuffle=False, num_workers=num_workers,\n",
    "        pin_memory=pin_memory, sampler=sampler\n",
    "    )\n",
    "\n",
    "    return loader\n",
    "\n",
    "def get_weighted_loader(data_dir, batch_size, weights, num_workers=3, pin_memory=False):\n",
    "    normalize = transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "    transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "    dataset = MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "    \n",
    "    sampler = WeightedRandomSampler(weights, len(weights), True)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset, batch_size=batch_size,\n",
    "        shuffle=False, num_workers=num_workers,\n",
    "        pin_memory=pin_memory, sampler=sampler\n",
    "    )\n",
    "\n",
    "    return loader\n",
    "\n",
    "def get_test_loader(data_dir, batch_size, num_workers=3, pin_memory=False):\n",
    "    normalize = transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "    transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "    dataset = MNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "    loader = DataLoader(\n",
    "        dataset, batch_size=batch_size,\n",
    "        shuffle=False, num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        out = F.relu(F.max_pool2d(self.conv2(out), 2))\n",
    "        out = out.view(-1, 320)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted, ground_truth):\n",
    "    predicted = torch.max(predicted, 1)[1]\n",
    "    total = len(ground_truth)\n",
    "    correct = (predicted == ground_truth).sum().double()\n",
    "    acc = 100 * (correct / total)\n",
    "    return acc.item()\n",
    "\n",
    "def train_transient(model, device, train_loader, optimizer, epoch, track=False):\n",
    "    model.train()\n",
    "    epoch_stats = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        acc = accuracy(output, target)\n",
    "        losses = F.nll_loss(output, target, reduction='none')\n",
    "        if track:\n",
    "            indices = [batch_idx*train_loader.batch_size + i for i in range(len(data))]\n",
    "            batch_stats = []\n",
    "            for i, l in zip(indices, losses):\n",
    "                batch_stats.append([i, l.item()])\n",
    "            epoch_stats.append(batch_stats)\n",
    "        loss = losses.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 25 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.2f}%'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "100. * batch_idx / len(train_loader), loss.item(), acc))\n",
    "    if track:\n",
    "        return epoch_stats\n",
    "    return None\n",
    "\n",
    "def train_steady_state(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    seen = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # forward pass and compute losses\n",
    "        output = model(data)\n",
    "        losses = F.nll_loss(output, target, reduction='none')\n",
    "\n",
    "        # compute importance weights\n",
    "        probas = (losses / losses.sum())\n",
    "        idxs = np.random.choice(len(data), len(data), p=probas.cpu().data.numpy())\n",
    "        seen.extend(list(idxs+batch_idx*train_loader.batch_size))\n",
    "\n",
    "        idxs = torch.from_numpy(idxs).long()\n",
    "        new_pdf = probas[idxs]\n",
    "        old_pdf = 1. / len(data)\n",
    "        weight = old_pdf / new_pdf\n",
    "\n",
    "        # resample\n",
    "        data_r = data.detach()[idxs]\n",
    "        target_r = target.detach()[idxs]\n",
    "\n",
    "        # forward pass\n",
    "        output_r = model(data_r)\n",
    "        acc = accuracy(output_r, target_r)\n",
    "        loss = F.nll_loss(output_r, target_r, reduction='none')\n",
    "\n",
    "        # reweight losses\n",
    "        loss = (loss * weight).mean()\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 25 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.2f}%'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "100. * batch_idx / len(train_loader), loss.item(), acc))\n",
    "\n",
    "    return seen\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_transient = 2\n",
    "num_epochs_steady = 3\n",
    "learning_rate = 1e-3\n",
    "mom = 0.99\n",
    "batch_size = 64\n",
    "normalize = False\n",
    "perc_to_remove = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "# instantiate convnet\n",
    "model = SmallConv().to(device)\n",
    "\n",
    "# relu init\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
    "\n",
    "# define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=mom)\n",
    "\n",
    "# instantiate loaders\n",
    "train_loader = get_data_loader(data_dir, batch_size, None, **kwargs)\n",
    "test_loader = get_test_loader(data_dir, 128, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.633006\tAcc: 10.94%\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 1.619076\tAcc: 48.44%\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.823937\tAcc: 71.88%\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.598104\tAcc: 79.69%\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.370156\tAcc: 92.19%\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.533670\tAcc: 81.25%\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.291023\tAcc: 89.06%\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 0.611064\tAcc: 82.81%\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.301691\tAcc: 85.94%\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.171518\tAcc: 96.88%\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.311606\tAcc: 92.19%\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.311102\tAcc: 90.62%\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.225483\tAcc: 93.75%\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.140306\tAcc: 93.75%\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.154456\tAcc: 95.31%\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.203898\tAcc: 93.75%\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.032903\tAcc: 100.00%\n",
      "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 0.218158\tAcc: 93.75%\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.071584\tAcc: 98.44%\n",
      "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 0.053365\tAcc: 100.00%\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.092851\tAcc: 96.88%\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.023382\tAcc: 100.00%\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.239355\tAcc: 93.75%\n",
      "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 0.159463\tAcc: 93.75%\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.054997\tAcc: 98.44%\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.179467\tAcc: 93.75%\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.061505\tAcc: 96.88%\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.169429\tAcc: 95.31%\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.159596\tAcc: 95.31%\n",
      "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 0.468967\tAcc: 87.50%\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.086294\tAcc: 95.31%\n",
      "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 0.220987\tAcc: 93.75%\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.208579\tAcc: 93.75%\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.264812\tAcc: 93.75%\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.046534\tAcc: 96.88%\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.117640\tAcc: 98.44%\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.196920\tAcc: 93.75%\n",
      "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 0.011510\tAcc: 100.00%\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9692/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.074488\tAcc: 98.44%\n",
      "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 0.132464\tAcc: 96.88%\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.066270\tAcc: 98.44%\n",
      "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 0.071151\tAcc: 98.44%\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.210089\tAcc: 96.88%\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.061824\tAcc: 98.44%\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.054427\tAcc: 96.88%\n",
      "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 0.140843\tAcc: 93.75%\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.093792\tAcc: 95.31%\n",
      "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 0.045398\tAcc: 98.44%\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.158572\tAcc: 95.31%\n",
      "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 0.026146\tAcc: 100.00%\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.226149\tAcc: 96.88%\n",
      "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 0.018054\tAcc: 100.00%\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.030244\tAcc: 100.00%\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.028830\tAcc: 98.44%\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.006937\tAcc: 100.00%\n",
      "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 0.154943\tAcc: 95.31%\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.044339\tAcc: 96.88%\n",
      "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 0.041290\tAcc: 98.44%\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.060451\tAcc: 98.44%\n",
      "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 0.015808\tAcc: 100.00%\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.101732\tAcc: 96.88%\n",
      "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 0.061473\tAcc: 98.44%\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.041257\tAcc: 98.44%\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.042018\tAcc: 98.44%\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.039075\tAcc: 98.44%\n",
      "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.040230\tAcc: 96.88%\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.066020\tAcc: 96.88%\n",
      "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 0.359166\tAcc: 87.50%\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.056128\tAcc: 98.44%\n",
      "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 0.159148\tAcc: 95.31%\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.183856\tAcc: 95.31%\n",
      "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 0.201649\tAcc: 95.31%\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.019636\tAcc: 100.00%\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.046323\tAcc: 98.44%\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.109396\tAcc: 96.88%\n",
      "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 0.015261\tAcc: 100.00%\n",
      "\n",
      "Test set: Average loss: 0.0628, Accuracy: 9803/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transient training\n",
    "losses = None\n",
    "for epoch in range(1, num_epochs_transient+1):\n",
    "    if epoch == 1:\n",
    "        losses = train_transient(model, device, train_loader, optimizer, epoch, track=True)\n",
    "    else:\n",
    "        train_transient(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[*] Effective Size: 54,000\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.093619\tAcc: 96.88%\n",
      "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 0.028880\tAcc: 98.44%\n",
      "Train Epoch: 2 [3200/60000 (6%)]\tLoss: 0.074904\tAcc: 95.31%\n",
      "Train Epoch: 2 [4800/60000 (9%)]\tLoss: 0.056242\tAcc: 96.88%\n",
      "Train Epoch: 2 [6400/60000 (12%)]\tLoss: 0.063272\tAcc: 98.44%\n",
      "Train Epoch: 2 [8000/60000 (15%)]\tLoss: 0.009403\tAcc: 100.00%\n",
      "Train Epoch: 2 [9600/60000 (18%)]\tLoss: 0.044018\tAcc: 100.00%\n",
      "Train Epoch: 2 [11200/60000 (21%)]\tLoss: 0.048966\tAcc: 98.44%\n",
      "Train Epoch: 2 [12800/60000 (24%)]\tLoss: 0.052409\tAcc: 98.44%\n",
      "Train Epoch: 2 [14400/60000 (27%)]\tLoss: 0.144705\tAcc: 98.44%\n",
      "Train Epoch: 2 [16000/60000 (30%)]\tLoss: 0.016330\tAcc: 100.00%\n",
      "Train Epoch: 2 [17600/60000 (33%)]\tLoss: 0.196836\tAcc: 95.31%\n",
      "Train Epoch: 2 [19200/60000 (36%)]\tLoss: 0.014758\tAcc: 98.44%\n",
      "Train Epoch: 2 [20800/60000 (39%)]\tLoss: 0.014360\tAcc: 100.00%\n",
      "Train Epoch: 2 [22400/60000 (41%)]\tLoss: 0.008776\tAcc: 100.00%\n",
      "Train Epoch: 2 [24000/60000 (44%)]\tLoss: 0.021482\tAcc: 98.44%\n",
      "Train Epoch: 2 [25600/60000 (47%)]\tLoss: 0.032960\tAcc: 100.00%\n",
      "Train Epoch: 2 [27200/60000 (50%)]\tLoss: 0.036666\tAcc: 100.00%\n",
      "Train Epoch: 2 [28800/60000 (53%)]\tLoss: 0.004326\tAcc: 100.00%\n",
      "Train Epoch: 2 [30400/60000 (56%)]\tLoss: 0.012220\tAcc: 100.00%\n",
      "Train Epoch: 2 [32000/60000 (59%)]\tLoss: 0.050762\tAcc: 98.44%\n",
      "Train Epoch: 2 [33600/60000 (62%)]\tLoss: 0.074806\tAcc: 96.88%\n",
      "Train Epoch: 2 [35200/60000 (65%)]\tLoss: 0.017976\tAcc: 100.00%\n",
      "Train Epoch: 2 [36800/60000 (68%)]\tLoss: 0.011366\tAcc: 100.00%\n",
      "Train Epoch: 2 [38400/60000 (71%)]\tLoss: 0.012571\tAcc: 100.00%\n",
      "Train Epoch: 2 [40000/60000 (74%)]\tLoss: 0.006263\tAcc: 100.00%\n",
      "Train Epoch: 2 [41600/60000 (77%)]\tLoss: 0.012048\tAcc: 100.00%\n",
      "Train Epoch: 2 [43200/60000 (80%)]\tLoss: 0.019819\tAcc: 98.44%\n",
      "Train Epoch: 2 [44800/60000 (83%)]\tLoss: 0.024453\tAcc: 98.44%\n",
      "Train Epoch: 2 [46400/60000 (86%)]\tLoss: 0.014094\tAcc: 100.00%\n",
      "Train Epoch: 2 [48000/60000 (89%)]\tLoss: 0.001373\tAcc: 100.00%\n",
      "Train Epoch: 2 [49600/60000 (92%)]\tLoss: 0.005566\tAcc: 100.00%\n",
      "Train Epoch: 2 [51200/60000 (95%)]\tLoss: 0.049989\tAcc: 96.88%\n",
      "Train Epoch: 2 [52800/60000 (98%)]\tLoss: 0.018555\tAcc: 98.44%\n",
      "\n",
      "Test set: Average loss: 0.0597, Accuracy: 9814/10000 (98%)\n",
      "\n",
      "\t[*] Effective Size: 48,600\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.000744\tAcc: 100.00%\n",
      "Train Epoch: 3 [1600/60000 (3%)]\tLoss: 0.005924\tAcc: 100.00%\n",
      "Train Epoch: 3 [3200/60000 (7%)]\tLoss: 0.007127\tAcc: 100.00%\n",
      "Train Epoch: 3 [4800/60000 (10%)]\tLoss: 0.000683\tAcc: 100.00%\n",
      "Train Epoch: 3 [6400/60000 (13%)]\tLoss: 0.027242\tAcc: 98.44%\n",
      "Train Epoch: 3 [8000/60000 (16%)]\tLoss: 0.003813\tAcc: 100.00%\n",
      "Train Epoch: 3 [9600/60000 (20%)]\tLoss: 0.003418\tAcc: 100.00%\n",
      "Train Epoch: 3 [11200/60000 (23%)]\tLoss: 0.008352\tAcc: 100.00%\n",
      "Train Epoch: 3 [12800/60000 (26%)]\tLoss: 0.000587\tAcc: 100.00%\n",
      "Train Epoch: 3 [14400/60000 (30%)]\tLoss: 0.000970\tAcc: 100.00%\n",
      "Train Epoch: 3 [16000/60000 (33%)]\tLoss: 0.089116\tAcc: 98.44%\n",
      "Train Epoch: 3 [17600/60000 (36%)]\tLoss: 0.002120\tAcc: 100.00%\n",
      "Train Epoch: 3 [19200/60000 (39%)]\tLoss: 0.005034\tAcc: 100.00%\n",
      "Train Epoch: 3 [20800/60000 (43%)]\tLoss: 0.002389\tAcc: 100.00%\n",
      "Train Epoch: 3 [22400/60000 (46%)]\tLoss: 0.016330\tAcc: 98.44%\n",
      "Train Epoch: 3 [24000/60000 (49%)]\tLoss: 0.006744\tAcc: 100.00%\n",
      "Train Epoch: 3 [25600/60000 (53%)]\tLoss: 0.001075\tAcc: 100.00%\n",
      "Train Epoch: 3 [27200/60000 (56%)]\tLoss: 0.000417\tAcc: 100.00%\n",
      "Train Epoch: 3 [28800/60000 (59%)]\tLoss: 0.000360\tAcc: 100.00%\n",
      "Train Epoch: 3 [30400/60000 (62%)]\tLoss: 0.000866\tAcc: 100.00%\n",
      "Train Epoch: 3 [32000/60000 (66%)]\tLoss: 0.006452\tAcc: 100.00%\n",
      "Train Epoch: 3 [33600/60000 (69%)]\tLoss: 0.024514\tAcc: 98.44%\n",
      "Train Epoch: 3 [35200/60000 (72%)]\tLoss: 0.001588\tAcc: 100.00%\n",
      "Train Epoch: 3 [36800/60000 (76%)]\tLoss: 0.009010\tAcc: 100.00%\n",
      "Train Epoch: 3 [38400/60000 (79%)]\tLoss: 0.003433\tAcc: 100.00%\n",
      "Train Epoch: 3 [40000/60000 (82%)]\tLoss: 0.001409\tAcc: 100.00%\n",
      "Train Epoch: 3 [41600/60000 (86%)]\tLoss: 0.000311\tAcc: 100.00%\n",
      "Train Epoch: 3 [43200/60000 (89%)]\tLoss: 0.010205\tAcc: 100.00%\n",
      "Train Epoch: 3 [44800/60000 (92%)]\tLoss: 0.000086\tAcc: 100.00%\n",
      "Train Epoch: 3 [46400/60000 (95%)]\tLoss: 0.002919\tAcc: 100.00%\n",
      "Train Epoch: 3 [48000/60000 (99%)]\tLoss: 0.005684\tAcc: 100.00%\n",
      "\n",
      "Test set: Average loss: 0.0654, Accuracy: 9822/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs_transient, num_epochs_steady+1):\n",
    "    losses = [v for sublist in losses for v in sublist]\n",
    "    sorted_loss_idx = sorted(range(len(losses)), key=lambda k: losses[k][1], reverse=True)\n",
    "    sorted_loss_idx = sorted_loss_idx[:-int((perc_to_remove / 100) * len(sorted_loss_idx))]\n",
    "    sorted_loss_idx.sort()\n",
    "    weights = [losses[idx][1] for idx in sorted_loss_idx]\n",
    "    if normalize:\n",
    "        max_w = max(weights)\n",
    "        weights = [w / max_w for w in weights]\n",
    "    train_loader = get_weighted_loader(data_dir, batch_size, weights, **kwargs)\n",
    "    print(\"\\t[*] Effective Size: {:,}\".format(len(train_loader.sampler)))\n",
    "    losses = train_transient(model, device, train_loader, optimizer, epoch, track=True)\n",
    "    test(model, device, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
